{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bzex9TLncn6s"
      },
      "source": [
        "Building GPT2 from scratch in PyTorch and training it on the whole Divina Commedia (plus some sonnets) to produce style-specific output (tercet of hendecasyllables)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zHGUiYbB5JOG"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "from collections import Counter\n",
        "from dataclasses import dataclass\n",
        "import inspect\n",
        "import math"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5eOhuGgO1vqi",
        "outputId": "7403a6f1-5b21-44c1-f315-fc6be2a6a46e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.11/dist-packages (0.8.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken) (2024.11.6)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.11/dist-packages (from tiktoken) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (2024.12.14)\n"
          ]
        }
      ],
      "source": [
        "#install library including the gpt2 tokenizer\n",
        "!pip install tiktoken"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PerV6SzF10vV"
      },
      "outputs": [],
      "source": [
        "import tiktoken"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SPYXdiUzuL9O"
      },
      "outputs": [],
      "source": [
        "#useful variables\n",
        "batch_size = 8 #can't increase because of Colab gpu usage limitations\n",
        "block_size = 1024\n",
        "max_iters = 5000\n",
        "eval_interval = 500\n",
        "eval_iters = 200\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "#hyperparameters replicate Karpathy tutorial, could also be determined through cross val\n",
        "#note: some of these might be overrun by loading the GPT2 weights directly from HuggingFace\n",
        "learning_rate = 3e-4\n",
        "n_emb = 400\n",
        "n_head = 10\n",
        "n_layers = 10\n",
        "dropout = 0.2\n",
        "patience = 1\n",
        "best_val_loss = float('inf')\n",
        "early_stop_counter = 0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vdSOuYpC6Iub"
      },
      "source": [
        "Loading dataset and tokenization"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gdown\n",
        "\n",
        "file_id = \"1qn_IL9W1ooscxWLPJAeUxNcvhCbcw1Xf\"\n",
        "url = f\"https://drive.google.com/uc?id={file_id}\"\n",
        "output = \"dante-corpus.txt\"\n",
        "gdown.download(url, output, quiet=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "id": "NaxQ1-_7oc4_",
        "outputId": "846e36da-f4fe-43f7-f04c-785081e7a219"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1qn_IL9W1ooscxWLPJAeUxNcvhCbcw1Xf\n",
            "To: /content/dante-corpus.txt\n",
            "100%|██████████| 562k/562k [00:00<00:00, 69.3MB/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'dante-corpus.txt'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vRN-4E5O0rs2",
        "outputId": "6937b6ce-d455-415b-9f8f-da850d9b27f3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of dataset in characters: 537063\n"
          ]
        }
      ],
      "source": [
        "with open('dante-corpus.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()\n",
        "print('Length of dataset in characters:', len(text))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LxYBRzqR6okF",
        "outputId": "68424fd4-f60d-4311-fec6-ea7e189bd07e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ima tremando si riscosse\n",
            "veggendo morto ’l cor nel lato manco.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(text[537000:])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J-nmx5IK2Wjl"
      },
      "outputs": [],
      "source": [
        "#load gpt2 encoder\n",
        "encoder = tiktoken.get_encoding(\"gpt2\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y4BlixRk5NiU",
        "outputId": "edc8b084-a008-4951-ad3a-eace098077b6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([238044]) torch.int64\n"
          ]
        }
      ],
      "source": [
        "data = torch.tensor(encoder.encode(text), dtype=torch.long)\n",
        "print(data.shape, data.dtype)\n",
        "vocab_size = encoder.n_vocab\n",
        "#print(data[:1000])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bFyhzupA55rP"
      },
      "outputs": [],
      "source": [
        "#split train/val\n",
        "n = int(0.9*len(data))\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "el_Ys7va8Bpg"
      },
      "source": [
        "Build model, load GPT2 and fine tune"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qDsZCJhVDsxP"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from dataclasses import dataclass\n",
        "\n",
        "#LoRA class for fine tuning\n",
        "class LoRA(nn.Module):\n",
        "    def __init__(self, in_features, out_features, rank):\n",
        "        super().__init__()\n",
        "        self.rank = rank\n",
        "        self.lora_a = nn.Parameter(torch.zeros(in_features, rank))\n",
        "        self.lora_b = nn.Parameter(torch.zeros(rank, out_features))\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        nn.init.kaiming_uniform_(self.lora_a, a=math.sqrt(5))\n",
        "        nn.init.zeros_(self.lora_b)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x @ self.lora_a @ self.lora_b\n",
        "\n",
        "class CausalSelfAttention(nn.Module):\n",
        "\n",
        "    def __init__(self, config, lora_rank=0):\n",
        "        super().__init__()\n",
        "        assert config.n_embd % config.n_head == 0\n",
        "        # key, query, value projections for all heads, but in a batch\n",
        "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd)\n",
        "        self.lora_attn = LoRA(config.n_embd, 3 * config.n_embd, lora_rank) if lora_rank > 0 else None\n",
        "        # output projection\n",
        "        self.c_proj = nn.Linear(config.n_embd, config.n_embd)\n",
        "        self.lora_proj = LoRA(config.n_embd, config.n_embd, lora_rank) if lora_rank > 0 else None\n",
        "\n",
        "        self.n_head = config.n_head\n",
        "        self.n_embd = config.n_embd\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n",
        "        # calculate query, key, values for all heads in batch and move head forward to be the batch dim\n",
        "        qkv = self.c_attn(x) + (self.lora_attn(x) if self.lora_attn else 0)\n",
        "        q, k, v = qkv.split(self.n_embd, dim=2)\n",
        "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
        "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
        "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
        "        y = F.scaled_dot_product_attention(q, k, v, is_causal=True) # flash attention\n",
        "        y = y.transpose(1, 2).contiguous().view(B, T, C) # re-assemble all head outputs side by side\n",
        "        # output projection\n",
        "        y = self.c_proj(y) + (self.lora_proj(y) if self.lora_proj else 0)\n",
        "        return y\n",
        "\n",
        "class MLP(nn.Module):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.c_fc    = nn.Linear(config.n_embd, 4 * config.n_embd)\n",
        "        self.gelu    = nn.GELU(approximate='tanh')\n",
        "        self.c_proj  = nn.Linear(4 * config.n_embd, config.n_embd)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.c_fc(x)\n",
        "        x = self.gelu(x)\n",
        "        x = self.c_proj(x)\n",
        "        return x\n",
        "\n",
        "class Block(nn.Module):\n",
        "\n",
        "    def __init__(self, config, lora_rank=0):\n",
        "        super().__init__()\n",
        "        self.ln_1 = nn.LayerNorm(config.n_embd)\n",
        "        self.attn = CausalSelfAttention(config, lora_rank=lora_rank)\n",
        "        self.ln_2 = nn.LayerNorm(config.n_embd)\n",
        "        self.mlp = MLP(config)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.attn(self.ln_1(x)) #skip connections\n",
        "        x = x + self.mlp(self.ln_2(x)) #skip connections\n",
        "        return x\n",
        "\n",
        "@dataclass\n",
        "class GPTConfig:\n",
        "    block_size: int = 1024 # max sequence length\n",
        "    vocab_size: int = 50257 # number of tokens\n",
        "    n_layer: int = 12 # number of layers\n",
        "    n_head: int = 12 # number of heads\n",
        "    n_embd: int = 768 # embedding dimension\n",
        "\n",
        "class GPT(nn.Module):\n",
        "\n",
        "    def __init__(self, config, lora_rank=0):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "\n",
        "        self.transformer = nn.ModuleDict(dict(\n",
        "            wte = nn.Embedding(config.vocab_size, config.n_embd),\n",
        "            wpe = nn.Embedding(config.block_size, config.n_embd),\n",
        "            h = nn.ModuleList([Block(config, lora_rank=lora_rank) for _ in range(config.n_layer)]),\n",
        "            ln_f = nn.LayerNorm(config.n_embd),\n",
        "        ))\n",
        "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
        "\n",
        "        # init params\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "        #weight sharing scheme\n",
        "        self.transformer.wte.weight = self.lm_head.weight\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, nn.Linear):\n",
        "            std = 0.02\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=std)\n",
        "            if module.bias is not None:\n",
        "                torch.nn.init.zeros_(module.bias)\n",
        "        elif isinstance(module, nn.Embedding):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.size()\n",
        "        assert T <= self.config.block_size, f\"Cannot forward sequence of length {T}, block size is only {self.config.block_size}\"\n",
        "        # forward the token and posisition embeddings\n",
        "        pos = torch.arange(0, T, dtype=torch.long, device=idx.device) # shape (T)\n",
        "        pos_emb = self.transformer.wpe(pos) # position embeddings of shape (T, n_embd)\n",
        "        tok_emb = self.transformer.wte(idx) # token embeddings of shape (B, T, n_embd)\n",
        "        x = tok_emb + pos_emb\n",
        "        # forward the blocks of the transformer\n",
        "        for block in self.transformer.h:\n",
        "            x = block(x)\n",
        "        # forward the final layernorm and the classifier\n",
        "        x = self.transformer.ln_f(x)\n",
        "        logits = self.lm_head(x) # (B, T, vocab_size)\n",
        "        loss = None\n",
        "        if targets is not None:\n",
        "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1))\n",
        "        return logits, loss\n",
        "\n",
        "    @classmethod\n",
        "    def from_pretrained(cls, model_type, lora_rank=None):\n",
        "        \"\"\"Loads pretrained GPT-2 model weights from huggingface, with optional LoRA integration.\"\"\"\n",
        "        assert model_type in {'gpt2', 'gpt2-medium', 'gpt2-large', 'gpt2-xl'}\n",
        "        from transformers import GPT2LMHeadModel\n",
        "        print(f\"Loading weights from pretrained GPT: {model_type}\")\n",
        "\n",
        "        # Define model-specific configuration\n",
        "        config_args = {\n",
        "            'gpt2':         dict(n_layer=12, n_head=12, n_embd=768),  # 124M params\n",
        "            'gpt2-medium':  dict(n_layer=24, n_head=16, n_embd=1024), # 350M params\n",
        "            'gpt2-large':   dict(n_layer=36, n_head=20, n_embd=1280), # 774M params\n",
        "            'gpt2-xl':      dict(n_layer=48, n_head=25, n_embd=1600), # 1558M params\n",
        "        }[model_type]\n",
        "        config_args['vocab_size'] = 50257  # Always 50257 for GPT model checkpoints\n",
        "        config_args['block_size'] = 1024  # Always 1024 for GPT model checkpoints\n",
        "\n",
        "        # Create a new GPT model instance with LoRA\n",
        "        config = GPTConfig(**config_args)\n",
        "        model = GPT(config, lora_rank=lora_rank)\n",
        "\n",
        "        # Load state_dict of the HuggingFace model\n",
        "        model_hf = GPT2LMHeadModel.from_pretrained(model_type)\n",
        "        sd_hf = model_hf.state_dict()\n",
        "\n",
        "        # Get the state_dict of our custom model\n",
        "        sd = model.state_dict()\n",
        "\n",
        "        # Exclude LoRA layers from pretrained state_dict keys\n",
        "        def is_lora_param(name):\n",
        "            return any(lora_key in name for lora_key in ['lora_a', 'lora_b'])\n",
        "\n",
        "        sd_keys = [k for k in sd.keys() if not k.endswith('.attn.bias') and not is_lora_param(k)]\n",
        "        sd_keys_hf = [k for k in sd_hf.keys() if not k.endswith('.attn.masked_bias') and not k.endswith('.attn.bias')]\n",
        "\n",
        "        # Ensure matching number of parameters (excluding LoRA layers)\n",
        "        assert len(sd_keys_hf) == len(sd_keys), f\"Mismatched keys: {len(sd_keys_hf)} != {len(sd_keys)}\"\n",
        "\n",
        "        # Handle Conv1D transposition for compatibility with HuggingFace weights\n",
        "        transposed = ['attn.c_attn.weight', 'attn.c_proj.weight', 'mlp.c_fc.weight', 'mlp.c_proj.weight']\n",
        "        for k in sd_keys_hf:\n",
        "            if any(k.endswith(w) for w in transposed):\n",
        "                # Special handling for Conv1D weights\n",
        "                assert sd_hf[k].shape[::-1] == sd[k].shape, f\"Shape mismatch for {k}\"\n",
        "                with torch.no_grad():\n",
        "                    sd[k].copy_(sd_hf[k].t())\n",
        "            else:\n",
        "                # Direct copy for non-transposed weights\n",
        "                assert sd_hf[k].shape == sd[k].shape, f\"Shape mismatch for {k}\"\n",
        "                with torch.no_grad():\n",
        "                    sd[k].copy_(sd_hf[k])\n",
        "\n",
        "        print(\"Weights loaded successfully. LoRA layers remain uninitialized and trainable.\")\n",
        "        return model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SFUfMy4_NhWv",
        "outputId": "56ac9bef-ac41-49e4-e67b-0dff65b26441"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading weights from pretrained GPT: gpt2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Weights loaded successfully. LoRA layers remain uninitialized and trainable.\n"
          ]
        }
      ],
      "source": [
        "model = GPT.from_pretrained('gpt2', lora_rank = 16)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bAGsNM2wNy1o"
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(1337)\n",
        "\n",
        "def get_batch(split):\n",
        "  data = train_data if split == 'train' else val_data\n",
        "  index = torch.randint(len(data) - block_size, (batch_size,))\n",
        "  x = torch.stack([data[i:i+block_size] for i in index])\n",
        "  y = torch.stack([data[i+1:i+block_size+1] for i in index])\n",
        "  x, y = x.to(device), y.to(device)\n",
        "  return x,y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VmbZ_BJ5Nrzs"
      },
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "  out = {}\n",
        "  model.eval()\n",
        "  for split in ['train', 'val']:\n",
        "    losses = torch.zeros(eval_iters)\n",
        "    for k in range(eval_iters):\n",
        "      X, Y = get_batch(split)\n",
        "      logits, loss = model(X, Y)\n",
        "      losses[k] = loss.item()\n",
        "    out[split] = losses.mean()\n",
        "  model.train()\n",
        "  return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EWujenSIN1Ag",
        "outputId": "5c07bae1-c653-4060-a2a7-f477d1a9885f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "125.324544 M parameters\n",
            "Step 0: train loss 5.390614, val loss 5.3659\n",
            "Step 500: train loss 2.773199, val loss 3.3514\n",
            "Step 1000: train loss 2.255894, val loss 3.5944\n",
            "Early stopping triggered. Stopping training.\n"
          ]
        }
      ],
      "source": [
        "m = model.to(device)\n",
        "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
        "\n",
        "#optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate) #add betas = (0.9, 0.95)\n",
        "# Freeze transformer parameters\n",
        "#for param in model.transformer.parameters():\n",
        "#    if param == model.transformer.ln_f.parameters():\n",
        "#      param.requires_grad = True\n",
        "#    else:\n",
        "#      param.requires_grad = False\n",
        "\n",
        "for name, param in model.named_parameters():\n",
        "    if 'lora' in name: # Check if the parameter belongs to lora\n",
        "        param.requires_grad = True\n",
        "    elif 'wte' in name:\n",
        "        param.requires_grad = True\n",
        "    elif 'lm_head' in name:\n",
        "        param.requires_grad = True\n",
        "    else:\n",
        "        param.requires_grad = False\n",
        "# Train only the lora layers, lm head and wte\n",
        "optimizer = torch.optim.AdamW([param for param in model.parameters() if param.requires_grad], lr=learning_rate)\n",
        "\n",
        "\n",
        "for iter in range(max_iters):\n",
        "\n",
        "  if iter%eval_interval == 0 or iter == max_iters -1:\n",
        "    losses = estimate_loss()\n",
        "    val_loss = losses['val']\n",
        "    print(f\"Step {iter}: train loss {losses['train']:4f}, val loss {losses['val']:.4f}\")\n",
        "\n",
        "    if val_loss < best_val_loss:\n",
        "      best_val_loss = val_loss\n",
        "      early_stop_counter = 0  # Reset counter if validation loss improves\n",
        "    else:\n",
        "      early_stop_counter += 1  # Increment counter if no improvement\n",
        "\n",
        "    #Check if we should stop early\n",
        "    if early_stop_counter >= patience:\n",
        "      print(\"Early stopping triggered. Stopping training.\")\n",
        "      break\n",
        "\n",
        "  xb, yb = get_batch('train')\n",
        "\n",
        "  logits, loss = model(xb, yb)\n",
        "  optimizer.zero_grad(set_to_none=True)\n",
        "  loss.backward()\n",
        "  #norm = torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "  optimizer.step()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U0oUm9InaXiS"
      },
      "outputs": [],
      "source": [
        "def generate(idx, max_new_tokens):\n",
        "    for _ in range(max_new_tokens):\n",
        "      idx_cond = idx[:, -block_size:]\n",
        "      logits, loss = m(idx_cond)\n",
        "      logits = logits[:,-1, :]\n",
        "      probs = F.softmax(logits, dim=-1)\n",
        "      idx_next = torch.multinomial(probs, num_samples=1)\n",
        "      idx = torch.cat((idx, idx_next), dim=1)\n",
        "    return idx"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#fine tune lora 16, lmhead and wte\n",
        "context = torch.tensor(encoder.encode('Nel mezzo del cammin di nostra vita'), dtype=torch.long, device=device).unsqueeze(0)\n",
        "print(encoder.decode(generate(context, max_new_tokens=500)[0].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-4KvC0goDTxy",
        "outputId": "0cab4a6c-7e6b-45bf-f66e-2ea775e9dfa8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Nel mezzo del cammin di nostra vita dal consiglio». Onde volsi,\n",
            "quando noi passai, e ’l maestro i sostegno,\n",
            "con sappi dentro a Dio con le piante.\n",
            "\n",
            "Non avea sparte ancora il popesmo\n",
            "pur a mano ad esso innanzi pien giorno,\n",
            "e Pistoia canta sentia ne brutta.\n",
            "\n",
            "Venimmo, spavam per sé stelle stelle\n",
            "d’esparea verso a sé del cielo ad alleggiorno,\n",
            "d’Mentore e cagion di trassel l’orreggiorno;\n",
            "\n",
            "indi mi sieni la fronte intra Orbis devi,\n",
            "né la testa via tue in sete tagliocco,\n",
            "dimmi officio in quella legge ogne errori.\n",
            "\n",
            "Quel quando fuor s’affaticar le fassai,\n",
            "guardommi elli al fin d’i Troiani;\n",
            "ma ’l maestro mio, che ’l dolor sì poco:\n",
            "\n",
            "si perché perché sì li altri si scossai,\n",
            "da qual ragioni a Vercelli etterno cui,\n",
            "e traggasi quel chiaro in tutti smorto.\n",
            "\n",
            "Pistoia di Dio mi fu detto e disfai\n",
            "carcato quel mar di Dio, se cotali\n",
            "(indi parler’ i Pfeiffer apparivanci\n",
            "del Palladier d’i Troiani e dal Palladier,\n",
            "\n",
            "quando mi parea gente non resti;\n",
            "batteggianar: «Benedetto in etterno\n",
            "ne l’alito e con campo d’errebbe’;\n",
            "batteggiano dir sotto i senti’ hoveli.\n",
            "\n",
            "Con abbracciato, e qua puoi dispoliti,\n",
            "e quando mi sostegno li fatti officiali».\n",
            "\n",
            "Allor’ a me pareggiando l’usato\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0bJaTZCok0HO",
        "outputId": "e836306a-8802-4d4f-82a6-1831f885ad00"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Nel mezzo del cammin di nostra vita dal consolar\n",
            "e di sangue, parlando privadi accraibo;\n",
            "dicendo ’l pietro s’intelletto testaro\n",
            "qua fossimo disïar con essere ivi’\n",
            "andava in me: «Però tespecciar sicarsa’;\n",
            "indi vid’ io l’altrimento a la mente’.\n",
            "\n",
            "Merrenti ancor così ron com’ io concilio\n",
            "dove sproni d’i pispone a la gente ruote\n",
            "che l’una facea la mia donna.\n",
            "\n",
            "Dio non salir lo scoglio mi dicere sgane,\n",
            "e con l’una riman, tra ’l salire sgane,\n",
            "e per lo scoglio m’ombra che volere.\n",
            "\n",
            "Poi gente altro uscì come il ventre rote\n",
            "dura, ch’uomo e forse vapor le membra\n",
            "quand’ io lo uscì come antico era.\n",
            "\n",
            "Nulla ruina, ch’è od oltre dolce leggera,\n",
            "contra ’l terzo epicicurta ond’ el partito,\n",
            "quivi e oramai; e quindi oramai osava;\n",
            "\n",
            "come quella donna sù, di spera gloria,\n",
            "a doppacciva fé levante dico era,\n",
            "ma viva ferro spezzato in la leggella!\n",
            "\n",
            "Con l’altra onde ci paura fummo il vecchio,\n",
            "quanto per paura e per passi moderno\n",
            "ove l’arco, del verbo de la dannia;\n",
            "\n",
            "e or dolce con campo d’erbe l’ovra\n",
            "per suo per le piante; e or s’avv’ io mosse,\n",
            "vidai Virgilio in me disprona;\n",
            "\n",
            "e or sévar l’addonar lo scoglio amaro,\n",
            "muoversi Cagnion d’una; e or\n"
          ]
        }
      ],
      "source": [
        "#fine_tune lora 8, lm_head, wte copied from previous versions for comparison\n",
        "context = torch.tensor(encoder.encode('Nel mezzo del cammin di nostra vita'), dtype=torch.long, device=device).unsqueeze(0)\n",
        "print(encoder.decode(generate(context, max_new_tokens=500)[0].tolist()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "goJIl8BzWHQQ",
        "outputId": "a047ec27-5148-4ae7-d9c2-c0b346794af1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Nel mezzo del cammin di nostra vita\n",
            "qual negligenza, rifonde\n",
            "su per lo monte che n’è gita.\n",
            "\n",
            "Anima fatta la destra costa già da lagrime\n",
            "sovr’ altrui sangue in natural vasello,\n",
            "per che dintorno suonin mille tube,\n",
            "\n",
            "chi move innanzi a la tua scïenza non è,\n",
            "che cotai colpi per essemproche fierlte;\n",
            "\n",
            "né credo che la vostra chiesa spense\n",
            "a chi stato, l’onor d’ingegno maìista,\n",
            "da un demonio già non si perde.\n",
            "\n",
            "Ella ruina in sì fatta parte verde;\n",
            "guarda’mi allor, come voi versi,\n",
            "gittati fia glorïosi di giunchi».\n",
            "\n",
            "«Oh, questi è quel ch’i’ v’ho scorte»,\n",
            "rispuose lui, «tegnon qui ne nel cista;\n",
            "però colà ciò vegnon qui ne l’aspetta.\n",
            "\n",
            "Tu hai sì presso a l’ombra che procò l’onta\n",
            "di quel ch’ebbe or così la lingua pronta,\n",
            "in voce cangia, e tra voi imposto\n",
            "da un discernal tutte altre ristette.\n",
            "\n",
            "Per le nove radici d’esto legno\n",
            "sovr’ altrui, ch’i’ ho veduto in parte\n",
            "de l’altro, Micòl amor che dentro a r giuggi;\n",
            "\n",
            "e noi movemmo l’assai suaisla e l’altro poli\n",
            "tutto, qual che si mostrò e ciascuna,\n",
            "per che ’l mio confortarmi nai quelleglia.\n",
            "\n",
            "Come al valor di che ci si mostra e\n",
            "quasi falcone ch’ancor de l’altra guancia\n",
            "e d’\n"
          ]
        }
      ],
      "source": [
        "#product of overfitting, training loss 0.6, fine tuning the full model, copied from previous version for comparison\n",
        "context = torch.tensor(encoder.encode('Nel mezzo del cammin di nostra vita'), dtype=torch.long, device=device).unsqueeze(0)\n",
        "print(encoder.decode(generate(context, max_new_tokens=500)[0].tolist()))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}